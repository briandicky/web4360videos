<!DOCTYPE HTML>
<!-- Website template by freewebsitetemplates.com -->
<html>
    <head>
        <meta charset="UTF-8">
        <title>Networking and Multimedia Systems Lab</title>
        <link rel="stylesheet" href="css/style.css" type="text/css">
    </head>
    <body>
        <div id="header">
            <div>
                <div class="logo">
                    <a href="index.html">NMSL@NTHU</a>
                </div>
                <ul id="navigation">
                    <li>
                        <a href="index.html">Home</a>
                    </li>
                    <li>
                        <a href="research.html">Research</a>
                    </li>
                    <li>
                        <a href="document.html">Document</a>
                    </li>
                    <li>
                        <a href="download.html">Download</a>
                    </li>
                    <li class="active">
                        <a href="faq.html">FAQ</a>
                    </li>
                    <li>
                        <a href="contact.html">Contact</a>
                    </li>
                </ul>
            </div>
        </div>
        <div id="contents">
            <h1>
                <b>Frequently Asked Questions</b>
            </h1>
            <ul>
                <img src="images/q_30x30.jpg"> Please, can you share with me the link to download the 360° Video Viewing Dataset (MMSys’17 paper)?
                <br></br>
                <img src="images/a_30x30.jpg"> The dataset is available online at <a href="https://nmsl.cs.nthu.edu.tw/360video/360dataset.zip">here</a>.
                <hr></hr>
                <br></br>

                <img src="images/q_30x30.jpg"> In your dataset, do you give the subjects enough space to turn around?
                <br></br>
                <img src="images/a_30x30.jpg"> In our experiments, all subjects are told to stand and given enough space to turn around when using HMD.
                <hr></hr>
                <br></br>

                <img src="images/q_30x30.jpg"> What is the shape of Field-of-View (FoV) you measured in the dataset paper?
                <br></br>
                <img src="images/a_30x30.jpg"> We mentioned that the HMD (Oculus Rift DK2) displays the current FoV, which is a fixed-size region, say 100x100 circle.
                <hr></hr>
                <br></br>

                <img src="images/q_30x30.jpg"> How do you generate the tiles, such as 192x192, 240x240 ,and 320x320?
                <br></br>
                <img src="images/a_30x30.jpg"> For all 360 videos, we divide each frame using H.264/AVC, which is mapped in equirectangular model, into 192x192 tiles, so there are 200 tiles in total. Then we number the tiles from upper-left to lower-right.
                <hr></hr>
                <br></br>

                <img src="images/q_30x30.jpg"> It is unclear why saliency map and motion map should be part of dataset itself. Can they be used to guarantee accurate timing between the dataset and origin data?
                <br></br>
                <img src="images/a_30x30.jpg"> The image saliency map that identifies the objects attracting the viewers’ attention the most and the motion map that high-lights the moving objects. We use both content and sensor data in our proposed fixation prediction network, a tile-based 360° video streaming server.
                <hr></hr>
                <br></br>

                <img src="images/q_30x30.jpg"> Saliency maps are computed based on equirectangular images using a classical image-based saliency mapping approach, there is no research that indicates this can be done. For CubeMap images it might not achieve same results using same saliency mapping software.
                <br></br>
                <img src="images/a_30x30.jpg"> There is no research that indicates the performance of the pre-trained CNNs using saliency map based on different projection models, such as equirectangular, cube and rhombic dodecahedron. A improved projection mechanism can be designed to mitigate the shape distortion caused by current projection models of 360° videos. It can make current saliency detection techniques more applicable for 360° videos.
                <hr></hr>
                <br></br>

                <img src="images/q_30x30.jpg"> How many public 360° videos datasets in the literature and what special features they have?
                <br></br>
                <img src="images/a_30x30.jpg"> To our best knowledge, there are 4 360° videos datasets have been published in ACM Multimedia Systems 2107 (MMSys'17). Here is the overview of each dataset, each of them contains different features.
                <img src="images/dataset_overview.jpg" style="margin-left: 200px;">
                <hr></hr>
            </ul>

            <br></br>

            <p>
            We need your helps and participate to improve our systems and algorithms. Please do not hesitate to <a href="contact.html">contact us</a> by sending an email. We will respond as fast as we can. Thank you. :)
            </p>
        </div>

        <div id="footer" align="center">
            <p style="margin-bottom: 0px">
            © 2017 NMSL@NTHU. All Rights Reserved.
            </p>
        </div>
    </body>
</html>

